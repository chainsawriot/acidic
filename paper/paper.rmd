---
title             : "Automation-coerced, increased dilution in correlation"
shorttitle        : "TO ERR IS CONTENT ANALYSIS; TO AUTOMATE, DILUTE"

author: 
  - name          : "Chung-hong Chan"
    affiliation   : "1"
    corresponding : yes
    address       : "A5, 6 (section A), 68159 Mannheim, Germany"
    email         : "chung-hong.chan@mzes.uni-mannheim.de"

affiliation:
  - id            : "1"
    institution   : "Mannheimer Zentrum für Europäische Sozialforschung, Universität Mannheim, Germany"
    
abstract: |
  Automated data-making methods in content analysis ---like all measurements--- are fallible. The purpose of this simulation study is to show this fallibility can lead to the correlation dilution effect: the biased estimation of true effect size towards zero, or, in other words, the unexpected reduction in statistical power. An alternative way to measure the performance of automated procedures, which focuses on the retention of statistical power, is proposed. This paper ends with best practices regarding planning, executing, and reporting of automated content analyses.
  
keywords          : "Automated Content Analysis, Correlation Dilution, Measurement Error, Statistical Power"
wordcount         : "6847"

bibliography      : "/home/chainsawriot/dev/dotfiles/bib.bib"

floatsintext      : yes
figurelist        : no
tablelist         : no
figsintext        : yes
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output:
 papaja::apa6_pdf:
   latex_engine: xelatex
---

# The leaderboard paradigm

Automated data-making procedures such as dictionary-based methods and supervised machine learning are introduced to content analysis to replace the labor-intensive manual coding process. In methodological studies, these procedures are an end in itself and many of these procedures have been shown to approximate human coding, the currently available gold standard. 

When mainstream communication scientists talk about a method "outperforms" another [e.g. @atteveldt:2021:VSA;@barberá:2020:atc], the yardstick is usually the same as the one used by computer scientists in machine learning benchmarks [@raji2021ai] such as SemEval or ImageNet Challenge: how more accurate does the method approximates human judgment than other methods. [^yard] In this so-called leaderboard paradigm [@ethayarajh2020utility], the performance of a method is captured by *performance metrics* such as F1, accuracy, precision, and recall. Previous studies that validate automated procedures for content analysis reported and ranked metrics such as F1 from various methods [e.g. @atteveldt:2021:VSA;@barberá:2020:atc;@dobbrick:2021:ETI]. The process of validation, as captured in @Grimmer2013's "validate, validate, validate" motto, is mostly about the whether the output of a procedure approximates human understanding of meanings in the source materials, otherwise known as semantic validation [@dimaggio:2013:E].

[^yard]: A minority of communication scientists argues also for the importance of alternative yardsticks, such as cost [@guo:2019:AFB], interpretability [@dobbrick:2021:ETI] and reproducibility [@chan:2020:REC].

Validation and revalidation of automated procedures are of paramount importance and many studies have so far demonstrated the domain specificity of many of these procedures [@GonzlezBailn2015;@dobbrick:2021:ETI]. @atteveldt:2018:wcm specify that "even if a researcher uses an existing off-the-shelf tool with published validity results it is vital to show how well it *performs* in a specific domain and on a specific task." (p. 87, emphasis added) I agree with @atteveldt:2018:wcm's conviction but propose not to look at the *performance* in the leaderboard paradigm. An alternative paradigm is proposed in the next section.

# The power retention paradigm

I propose to look at the performance of automated procedures in an alternative paradigm called "(statistical) power retention" paradigm. This paradigm doesn't deal with how close the automated procedures approximate human coding *directly*. Instead, the power retaining paradigm deals with how effective automated procedures facilitate hypothesis testing by keeping the expected statistical power.

In a problem-driven content analysis [@krippendorff2018content] [^contentanalysis], a data-making procedure such as manual coding or supervised machine learning is not an end in itself, but a means to meet an end. Quantitative Content analysis is always deductive, i.e. the ultimate goal of a content analysis is the theory-testing deductive procedure based on statistical inference (e.g. hypothesis testings) *after* the coding procedure [@krippendorff2018content]. Therefore, the actual performance of an automated procedure for content analysis should *not* be measured by how close the automated procedure approximates human coding, but by how useful the automated procedure is to faciliate the downstream tasks such as hypothesis testing [^vaccine].

[^contentanalysis]: Please note that this article deals exclusively with (automated) content analysis with a problem-driven design, which by definition always aims to deductively answer some research questions [@krippendorff2018content]. There are also other designs: text-driven design (also known as qualitative content analysis) and method-driven design for justifying methods. Outside the realm of content analysis, less stringently defined approaches such as "text-mining", "text-as-data" approach, "QTA" (quantitative text analysis), "NLP" (natural language processing) or even marketing terms such as Big Data or AI analyses might not be deductive. "Text-mining", for instance, implies "mining" (or uncovering) patterns in data and thus should be entirely inductive. 

[^vaccine]: An analogy to this is the performance (or efficacy) of a new vaccine is measured by how useful it is to bring benefits to the patients, not by how close the new vaccine approximates the pharmacokinetic of the "gold standard", or any, existing vaccine.

## Correlation dilution in content analysis

How can data-making procedures impact the subsequent hypothesis testing in a content analysis? To answer this question, one has to first understand what threats the validity of hypothesis testing. Sampling error and measurement errors are two of the most common threats. Both random and systematic sampling error has received much more attention by content analysts [e.g. the calculation of sample size beforehand and ensure the representativeness of the sample by random sampling, see the best practice article by @lacy:2015:IBP] and thus are not repeated here. Measurement errors, on the other hand, does not receive a lot of attention [except e.g. @bachl:2017:CME;@krippendorff:2011:AIR]. Even the reporting of reliability is now a standard procedure, the relationship to hypothesis testing is not explicitly explained [except @geiss2021statistical, more on this later].

Applying the Classical test theory, the code of an article is an observed variable ($O$) and this observed result is a combined result of the unobserved true score ($T$), and measurement errors ($E$, i.e. $O = T + E$). The instrument used to obtain the observation $O$ is *fallible* and such measurement errors $E$ are unavoidable. And there are two components of measurement errors: random measurement error and systematic measurement error. Random measurement error, as the name implies, is the *unpredictable* variation and would only influence the variance of a variable but not the average level. The direction of influence is, however, predictable as random measurement error always increases the variance. In content analysis, interrater variability is a measurement error that is usually assumed to be random because coders are assumed to be interchangeable [@bachl:2017:CME;@krippendorff:2011:AIR].

On the contrary, systematic measurement error is the *predictable* variation that can be explained by systematic factors. One example of this systematic measurement error is mixing German and French articles together and training two different classification models for each individual language [@gilardi:2021:SMP]. If the prediction models are different in performance, e.g. German articles are classified less accurately than French articles, the measurement error in this case is predictable by a systematic factor of whether or not the content is in German. Systematic measurement error could influence both the variance and the average level of a variable, but the direction of influence depends on the composition of data.

Let's make an unrealistic assumption that systematic measurement error is negligible. The direct consequence of doing statistical tests with observations containing random measurement error is the unexpected increase in variance. This inflated variance can lead to two related consequences: 1) the effect size estimation (e.g. correlation coefficient) is biased towards zero [^cohend] and 2) when the sample size and $\alpha$ (Type I error rate) remain the same and the estimated effect size is biased towards zero, it unexpectedly increases $\beta$ (Type II error rate). Figure \@ref(fig:fig1) displays a simulation of this phenomenon. A correlated data of true $X$ and $Y$ are generated with a preset Pearson's correlation of 0.7. Then some random noise is added to $X$ to simulate the observation of $X$ with random measurement error. In the density curve, the observed $X$ with random measurement error has the same peak as the true $X$ but with a much broader spread, i.e. a higher variance. When compared with the true correlation, the correlation between the observed $X$ and $Y$ is biased towards zero, as indicated by the much flatter regression trend line.

```{r fig1, fig.height = 3, fig.cap = "A simulation of correlation dilution: the inflated variance when observations containing random measurement error (Left) and the weaken correlation when observations containing random measurement error. (Right)"}
require(ggplot2)
suppressMessages(require(magrittr))
suppressMessages(require(tidyverse))
require(cowplot)
require(MASS)

set.seed(1212111)
sigma <- rbind(c(1, 0.7), c(0.7, 1))
mu <- c(10, 5)

x <- as.data.frame(mvrnorm(n = 200, mu = mu, Sigma = sigma))

distx <- x[,1] + rnorm(200)
## hist(x[,1])
## hist(distx)

alldata <- rbind(x, data.frame(V1 = distx, V2 = x[,2]))
alldata$type <- as.factor(c(rep("No error", 200), rep("Error", 200)))

alldata %>% ggplot(aes(x = V1, fill = type, group = type)) + geom_density(adjust = 1.5, alpha=.4) + theme_minimal() + xlab("X") + scale_fill_brewer(palette="Dark2") + theme(legend.position="bottom") -> spread

alldata %>% ggplot(aes(y = V2, x = V1, col = type, group = type)) + geom_point(alpha = .4) + geom_smooth(aes(group=type), method = "lm", formula = 'y ~ x', alpha = .4, se = FALSE) + theme_minimal() + xlab("X") + ylab("Y") + scale_color_brewer(palette="Dark2") + theme(legend.position="bottom") -> scatter

plot_grid(spread, scatter)
```

This phenomenon has long been discovered by statisticians [e.g. @spearman:1904:PMA] and goes with different names, e.g. correlation dilution, correlation attenuation, attenuation bias, and measurement error bias. In this paper, I am going to call it correlation dilution [^correlation].

[^cohend]: The most intuitive example is the calculation of Cohen's D, which is calculated as the standardized mean difference: $d = \frac{\bar{x}_1 - \bar{x}_2}{s}$. When the mean difference (numerator) remains constant, a larger variance increases the sample standard deviation $s$ (denominator) and thus makes the effect size bias towards zero.

[^correlation]: Please note that I have restricted the discussion to correlation but not regression; effect size but not regression coefficients. A related phenomenon called regression dilution is slightly more complicated. Similar to correlation dilution, the effect size of a regression model ($R^{2}$) on observations with measurement errors is also unexpectedly biased towards zero. When it's the independent variable with measurement errors, the slope of the regression model is also biased towards zero. But the slope of the regression model is not biased towards zero, if it's only the dependent variable with measurement errors [@fuller2009measurement].

For content analysis, @geiss2021statistical demonstrates the problem of correlation dilution with a series of Monte Carlo simulations, although the problem is not called correlation dilution in the paper. Coding accuracy (such as coder agreement), if one treats it as a random measurement error, can also induce the correlation dilution effect in content analysis. For example, the observed correlation ($\hat{r}$) in a content analysis, according to @geiss2021statistical's simulations, can be expressed using the following sigmoid function

\begin{align}
  \hat{r} = \frac{1 \cdot | \rho |}{1 + e^{(-(b+d \cdot | \rho |)) \cdot (Q - c))}}
\end{align}

where $\rho$ is the true effect size, $Q$ is the coding accuracy and $b$, $c$, and $d$ are some constants. The denominator gets bigger when $Q$ gets smaller, which results in the observed correlation getting weaker also. With this important finding, @geiss2021statistical suggests that one should "consider coder agreement, sample size, and effect size in conjunction" (P. 86). @geiss2021statistical situates his simulations in the realm of content analysis in general and automated content analysis is also a kind of content analysis. In the next section, I will talk about an issue that is unique to automated content analysis.

## Automation-coerced, Increased Dilution in Correlation

Manual coding is an inevitable fallible procedure. Automated procedures such as dictionary-based methods or supervised machine learning are either validated with or trained on data generated by the fallible manual coding procedure. As manually coded data are considered to be the "gold standard" benchmark, automated procedures cannot have an accuracy rate larger than 100\% and are assumed to be inherently "worse" than manual coding procedures. The disagreement between manual coding and automated procedures, which is also inevitable, creates another source of fallibility. The relationship between the reliability of manual coding procedure and the subsequent validation process of automated procedures has been demonstrated by @song:2020:ivw. Concretely, the quality of human-coded data used for validating automated procedures are rarely checked and the quality can bias accuracy metrics such as F1.

The end products of automated procedures are what @knox:2022:TCT called "(imperfect) learned proxies" [^proxy]. In the sentiment analysis scenario, for instance, the surrogate of "sentiment" is learned from the textual content. Except in a handful of articles in which the actual sentiment are manually observed to serve as the so-called "gold standard", we never "observe" any sentiment. Instead, we infer the sentiment from the observed textual content using a certain prediction model. If having measurement error means imperfect in @knox:2022:TCT's sense, then the end product of the automated procedure using in an automated content analysis should be called "imperfect imperfect surrogate". It is because the original "gold standard" is known to be imperfect too [@song:2020:ivw;@geiss2021statistical]. An automated procedure based on a prediction model with 100% accuracy in predicting the imperfect human coding is only equivalent to the imperfect human coding. But in reality, the accuracy of the prediction model ---the "performance" of a model in the leaderboard paradigm--- can hardly be 100%. Therefore, automated procedures contain two sources of measurement error: 1) the inaccuracy of the prediction model to predict human understand and; 2) the inherent interrater disagreement. Two wrongs (measurement errors) certainly don't make one right. Instead, having two sources of measurement error inflates the variance further and dilutes the true correlation even more. This **a**utomation-*i*nduced, **c**oerced **di**lution *in* **c**orrelation (ACIDIC) is probably unique to automated content analysis. @knox:2022:TCT observe that it is a "common practice of conflating proxies with the underlying true concept". I echo this sentiment. I rereviewed the 37 communication research articles reviewed by @song:2020:ivw of having validated automated content analysis, none of the research papers acknowledges this ACIDIC problem.

[^proxy]: This is a direct quotation of the term, not an endorsement. It is important to note that the end products of automated content analysis should not be called proxies because a proxy variable is defined as "a variable that is used in place of one that **cannot** be measured" [@upton2014, emphasis added]. In the case of automated content analysis, one **can** certainly code the materials manually. Perhaps out of scalability concerns one doesn't want to code the materials manually. A more proper term for this should be "surrogate" which is defined as "a variable that can be measured (or is easy to measure) that is used in place of one that cannot be measured (or is difficult to measure)" [@upton2014].

## Simulate ACIDIC in automated content analysis and measure performance holistically (pH)

Similar to the studies by @geiss2021statistical, @song:2020:ivw, and @bachl:2017:CME, this paper is going to study ACIDIC using Monte Carlo simulation. Building on top of @geiss2021statistical, the observed point and interval estimations of the observed correlation from an automated content analysis are governed by:

1. True effect size ($\rho$)
2. Sample size of the entire analysis ($n$)
3. Type I error rate ($\alpha$)
4. Predictive accuracy ($\zeta$)
5. Coder accuracy when coding gold standard for validation ($Q$)

Using only the items 1, 2, and 3, one can also calculate the expected statistical power ($1 - \beta$). Due to the ACIDIC induced by items 4 and 5, one would anticipate a reduction in statistical power. Therefore, the actual performance of an automated procedure in content analysis should be measured by how much expected statistical power it can retain. The power retaining performance is a holistic measurement of all 5 items in the above list; not just the item 4 as in the leaderboard paradigm. If an automated procedure cannot retain enough statistical power, it can't facilitate the deductive procedure and shouldn't be deployed for the purpose of automated content analysis. I will demonstrate how to measure the performance in the power retaining paradigm later in this article.

# Methods

The purpose of the Monte Carlo simulation is to study how the all five items specified above ($\rho$, $n$, $\alpha$, $Q$, and $\zeta$) influence the observed effect size. There are many types of data one could study in a content analysis. In this simulation, the simplest case of $2 \times 2$ is used: there is a binary exposure variable $X$ (e.g. being uncivil or not) and a binary outcome variable $Y$ (e.g. being shared or not). In this simulation, it is assumed that $Y$ can be measured without any measurement error. $X$ is only manually coded in a handful of the articles for training the prediction model, as well as evaluating the out-of-sample accuracy of the prediction. For the entire dataset, $X$ is the output from the prediction model trained on some other observed variables (e.g. textual materials). <!-- In the online appendix, the simulation using $X$ with an ordinal scale is provided. The scale of measurement does affect the possible range of variance, but it does not change the fact that ACIDIC exists. Also, --> Reversing the $X$ and $Y$, i.e. the outcome variable is learned but not the exposure variable, will not change the result because this simulation only looks at observed effect sizes. However, when both $X$ and $Y$ are learned that would change the observed effect size drastically. However, it is uncommon for both exposure and outcome variables to be learned in an automated content analysis.

## True effect size ($\rho$)

Similar to @geiss2021statistical, the correlation coefficient $R$ is used. It is because most readers will be familiar with $R$. Although $R$ presupposes interval- or ratio-level data, it is still possible to calculate an effect size that is equivalent to $R$ from a $2 \times 2$ table. For this purpose, I calculated the mean square contingency coefficient, $\phi$ [@yule:1912:MMA]. This value is used as $\rho$ and simulated data were generated with a prespecified level of $\rho$. In the following sections, $\rho$ denotes the true effect size and $R$ denotes the observed effect size for clarity.

The method to generate $2 \times 2$ data with a specific $\rho$ is first selecting a random number from the uniform distribution $\mathcal{U}_{[0.1, 0.5]}$ to denote the probability of the outcome, i.e. $P(Y=1)$. Then, another random number from the uniform distribution $\mathcal{U}_{[0.01, 0.5]}$ is selected to denote the probability of the outcome in the unexposed group, i.e. $P(Y=1|X=0)$. With the two probabilities $P(Y=1)$ and $P(Y=1|X=0)$, the probability of the outcome in the exposed group, i.e. $P(Y=1|X=1)$, is found by the bisection method [@corliss:1977:WRD] such that $\phi$ is equal to the prespecified level with less than 0.01 discrepancy. 

With $P(Y=1)$, $P(Y=1|X=0)$, $P(Y=1|X=1)$, and the total sample size, simulated data are generated. Similar to @geiss2021statistical, simulated data with the following levels of $\rho$ are generated: 0, .05, .1, .15, .2, .3, .4, .5, and .75. It is important to note that (observed) effect sizes found in communication research are usually in the lower end of this spectrum [@rains:2018:S]. Effect sizes beyond .75 are extremely rare empirically.

## Sample size of the entire analysis ($n$)

The following sample sizes are selected: 500, 1 000, 10 000, and 30 000. The upper limit is substantially larger than the one selected by @geiss2021statistical (1 000) because automated content analysis usually has a much larger sample size. This range also covers most of the studies reviewed by @song:2020:ivw. However, it is important to note that these are *sample sizes*, not *population sizes*. Automated content analytic studies with a data size larger than 30 000 usually study the entire population [e.g. @su:2018:U with a whopping data size of 243 235 637]. In those cases, frequentist inference does not apply [@western:1994:BIC] even though p-values are still calculated in those population studies for no purpose.

## Type II error rate ($\alpha$)

The generally accepted level of 0.05 is selected.

## Predictive accuracy ($\zeta$)

In this simulation, $\zeta$ is assumed to be measured in the validation procedure. In the machine learning literature, it is confusingly called "test accuracy" (the accuracy in the test set). Here, I use the term "out-of-sample accuracy" from the statistical forecasting literature. The data used for validating the model are assumed to be coded similarly to the training samples and the data used for calculating $Q$.

There are many ways to report the predictive accuracy of a model. The most commonly used metrics are Correct Classification Rate (CCR, also known simply as "accuracy") and F1. All of these measurements can be broken down into true positive rate ($\zeta_{+}$), true negative rate ($\zeta_{-}$) and prevalence of exposure ($P(X = 1)$). CCR can be expressed as:

\begin{align}
  CCR &= P(X = 1) \times \zeta_{+} + (1 - P(X = 1)) \times \zeta_{-}
\end{align}

F1 can be expressed as the harmonic mean of $\zeta_{+}$ and positive predictive value (PPV, also known as "precision").

\begin{align}
  \label{eq:ppv}
  F1 &= \frac{2 \times (\zeta_{+} + PPV)}{\zeta_{+} \times PPV}
\end{align}

Using the Bayes rule, PPV can be expressed as a function of $\zeta_{+}$, $\zeta_{-}$, and $P(X = 1)$.

\begin{align}
  PPV &= \frac{\zeta_{+} \times P(X = 1)}{\zeta_{+} \times P(X = 1) + (1 - \zeta_{-}) \times (1 - P(X = 1))}
\end{align}

Similar to the case of $\rho$, random combinations of $\zeta_{+}$ and $\zeta_{-}$ were generated with the fixed $P(X = 1)$ from the simulated data the specific level of CCR or F1. The following levels of F1 and CCR are selected: 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.9.

## Coder accuracy ($Q$)

The data used for calculated $Q$ are assumed to be coded similarly to the training samples and the data used for calculating $\zeta$.

As the impact of coder accuracy to the observed effect size has been well studied by @geiss2021statistical, the current simulation selects only one level, .7. Similar to the "Equality Distribution" guessing mode, $Q$ of .7 means 70\% recognition and 30\% guessing. When it is needed to guess a binary answer, it is based on a simulated coin flip. Therefore, for the 30\% guessed answers, there are still 50% chance to be correct ---what @krippendorff:2011:AIR calls "agreement by chance".

# Simulation

The entire simulation used nine levels of $\rho$, four levels of $n$, two different accuracy metrics, eight levels of accuracy, one level of $Q$ and one level of $\alpha$. In total, there are $9 \times 4 \times 2 \times 8 = 576$ scenarios. For each scenario, 1 000 simulation runs were made.

Each simulation run, which represents one empirical study, contains the following steps:

1. Generate $n$ pairs of $X$ and $Y$ with $P(Y=1)$, $P(Y=1|X=0)$, $P(Y=1|X=1)$ derived from the specific $\rho$
2. Manipulate $X$ with $\zeta_{+}$ and $\zeta_{-}$ derived from either the prespecified F1 or CCR. If $X = 1$, this $X$ has $1 - \zeta_{+}$ chance to flip to 0. Similarly, if $X = 0$, this $X$ has $1 - \zeta_{-}$ chance to flip to 1.
3. Manipulate $X$ again with $Q$ using the "Equality Distribution" guessing mode [@geiss2021statistical]
4. Calculate $R$ between the manipulated $X$ and $Y$

For each scenario, 1000 observed effect sizes were calculated. The $50^{th}$ (median), $2.5^{th}$, and $97.5^{th}$ percentiles of these observed effect sizes was calculated. The latter two correspond to the upper and lower limits of interval estimation when $\alpha = 0.05$.

# Results

Figure 2 (F1) and Figure 3 (CCR) show the simulation results. In general, ACIDIC is evidenced in all cases. Take $\rho = .2$ as an example, the $97.5^{th}$ percentile of $R$ never reaches .2. Even with F1 = 0.9 (the accuracy level of some state-of-the-art machine learning model) and a large sample size, the upper limit of $R$ is only around .1.

Similar to @geiss2021statistical, $n$ does not change the trajectory of the relationship between median $R$ and $\zeta$ when $\rho$ is constant. $n$ only changes the interval estimation. A higher $n$ produces a narrower interval estimation, which guards against committing Type II errors.

The simulation also shows that $\zeta$ does not change the Type I error rate, a similar finding to @geiss2021statistical. It is evidenced by the flat line in all cases where $\rho = 0$, i.e. the null hypothesis is actually true. However, $\zeta$ does change the Type II error rate. When $\rho \neq 0$ (null hypothesis is not true), for instance $\rho = .2$, and $F1 \le .80$, zero is still included in the interval estimation in all studied $n$. As a matter of fact, models with $F1 \le 0.80$ cannot prevent Type II error, even when $\rho$ is unrealistically large (0.75) and $n$ is 30 000.

```{r fig2, fig.height = 5.5, fig.cap = "The relationship between F1 (X-axis) and observed effect size (Y-axis) by true effect size (column) and sample size (row). The line represents the median of all simulated observed effect sizes and the ribbon represents its 95% interval estimation."}
scenarios <- readRDS(here::here("analysis/f1_scenarios.RDS"))
scenarios %>% mutate(mdn = purrr::map_dbl(res, ~quantile(.$d2_phi, probs = 0.5, na.rm = TRUE)), pct25 = purrr::map_dbl(res, ~quantile(.$d2_phi, probs = 0.025, na.rm = TRUE)), pct975 = purrr::map_dbl(res, ~quantile(.$d2_phi, probs = 0.975, na.rm = TRUE))) %>% ggplot(aes(y = mdn, ymin = pct25, ymax = pct975, x = z)) + geom_line() + geom_ribbon(alpha = 0.5) + facet_grid(ss~es) + theme_minimal() + xlab("F1") + ylab("Observed effect size (R)") + theme(axis.text.x = element_text(angle = 90)) + ylim(-.3, +.5)
```

```{r fig3, fig.height = 5.5, fig.cap = "The relationship between correct classification rate (X-axis) and observed effect size (Y-axis) by true effect size (column) and sample size (row). The line represents the median of all simulated observed effect sizes and the ribbon represents its 95% interval estimation."}
scenarios <- readRDS(here::here("analysis/acc_scenarios.RDS"))
scenarios %>% mutate(mdn = purrr::map_dbl(res, ~quantile(.$d2_phi, probs = 0.5, na.rm = TRUE)), pct25 = purrr::map_dbl(res, ~quantile(.$d2_phi, probs = 0.025, na.rm = TRUE)), pct975 = purrr::map_dbl(res, ~quantile(.$d2_phi, probs = 0.975, na.rm = TRUE))) %>% ggplot(aes(y = mdn, ymin = pct25, ymax = pct975, x = z)) + geom_line() + geom_ribbon(alpha = 0.5) + facet_grid(ss~es) + theme_minimal() + xlab("CCR") + ylab("Observed effect size (R)") + theme(axis.text.x = element_text(angle = 90)) + ylim(-.3, +.5)
```

## Measuring performance in the power retention paradigm

Suppose the expected true effect size $\rho$ of a study is .0852 and $n$ is 2019. In the validation study, the coder accuracy, as indicated by Krippendorf's $\alpha$, is .8964. The $\zeta_{+}$ and $\zeta_{-}$ of the trained prediction model are .8310 and .7210 respectively. Suppose the Type I error rate is 0.05.

In order to calculate the observed statistical power, a Monte Carlo simulation is performed and each simulation run contains the following steps:

1. Generate $n$ pairs of $X$ and $Y$ with $P(Y=1)$, $P(Y=1|X=0)$, $P(Y=1|X=1)$ derived from the specific $\rho$
2. Manipulate $X$ with $\zeta_{+}$ and $\zeta_{-}$ derived from either the prespecified F1 or CCR. If $X = 1$, this $X$ has $1 - \zeta_{+}$ chance to flip to 0. Similarly, if $X = 0$, this $X$ has $1 - \zeta_{-}$ chance to flip to 1.
3. Manipulate $X$ again with $Q$ using the "Equality Distribution" guessing mode [@geiss2021statistical]
4. Calculate $R$ between the manipulated $X$ and $Y$
3. Calculate the (two-sided) P-value of $R$

$\beta$ is equal to the total number of runs with $P > \alpha$ divided by the total number of runs. And the observed power ($1 - \beta$) in this case is 41.1\%. 

Then, the expected statistical power is calculate. It is calculated by substituting $\zeta_{+}$, $\zeta_{-}$, and $Q$ with 1. For most of the cases in automated content analysis, one can expect this power to be 100\% and this step is not needed in practice [^power].

[^power]: For $\rho = .2019$, $n > 106$ would have 100\% power. For a corpus of $n \le 106$, it would be better off using manual coding, which only suffers from coding errors. If it is really needed, this can also be derived analytically using the central limit theorem because the distribution of $R$ approximates normal when $R$ is measured without measurement errors, i.e. $\mathcal{N}(\rho, \sqrt{\frac{1 - \rho^{2}}{n}})$.

By comparing the observed and expected statistical powers, the statistical power reduces to 41.1\% only due to ACIDIC. That is also the statistical power retained by the automated procedure. A lower power retention indicates less fit for use in content analysis. This retained power, dubbed $\Phi$, can be calculated using the R function `cal_bigphi` provided (https://osf.io/vhwka/?view_only=84fda38794294dee8c34a2e563024045).

```r
cal_bigphi(rho = 0.0852, n = 2019,
           tpr = 0.831, tnr = 0.721,
           Q = 0.8964)
```

# Conclusion

The simulation study in this paper demonstrates the correlation dilution induced by the random measurement errors associated with automated procedures. This ACIDIC problem can drag down the planned statistical power. An R function is provided to calculate the power retained by automated procedures. The retained power $\Phi$ is a holistic measurement of many factors: $\rho$, $n$, $\alpha$, $\zeta$, and $Q$ [^cutoff]. For the purpose of automated content analysis, $\Phi$ is more meaningful than metrics such as F1 and CCR, which are used in the so-called leaderboard paradigm [@ethayarajh2020utility] which focus only on $\zeta$ and are often reported in communication research [@song:2020:ivw].

[^cutoff]: I refrain from suggesting a cut-off point for $\Phi$, not least because all cut-off points are controversial. Suppose you are a security analyst and world peace is at stake. You need to find, via automated content analysis, the peace-making signals in secret government cables from a geopolitical adversary. A mutual nuclear destruction will be triggered if a Type II error is committed. Would you accept a nice looking $\Phi = 0.80$ for this task? Many books teach us to calculate sample size with 80\% power, right? Remember, there is a 20\% chance of mutual nuclear destruction.

Having said so, $\Phi$ is not longer meaningful when $n$ is getting too high. Any combination with $\rho \ne 0$, $\alpha > 0$, $\zeta_{+} > 0.5$ , $\zeta_{-} > 0.5$, and $Q > 0$ produces a 100\% $\Phi$, when $n$ approaches positive infinity. It also appears that a straightforward solution to maximize the retained statistical power is to increase $n$, rather than improve either $\zeta$ or $Q$. First of all, this practice is not to "retain" more statistical power, but to **ob**tain. A procedure can only **re**tain more power than another procedure when comparing the two procedures with the same $n$, $\alpha$, and $\rho$.

There is no shortage of digital data for us to analyze [remember the whopping data size of @su:2018:U]. These $n$ articles do not need to be manually coded and therefore there is virtually no cost to analyze more digital data to obtain more statistical power. Is it fine to obtain almost infinite statistical power by boosting *n*?

As found in the current Monte Carlo simulation (as well as predicted by the law of large numbers), a higher $n$ certainly reduces the chance of Type II error when $\zeta$, $Q$, $\rho$, and $\alpha$ are constant. However, the point estimate of the effect size, as indicated by the median estimates in Figure 2 and Figure 3, do not change with $n$. An extremely bad predictive model with low $\zeta$ and $Q$ still dilutes the correlation. When the null hypothesis is not true, astronomical sample sizes can reduce the chance of obtaining a zero effect size. But they can't change the diluted correlation and make it even more certain to obtain a diluted correlation. **If the goal is to obtain an unbiased estimation of the true effect size, the only way is to improve both $\zeta$ and $Q$.** For population studies such as @su:2018:U, this point is even more important because P-values therefrom are meaningless. The primary goal should be to estimate unbiased effect sizes. For those studies, it is more imperative to improve both $\zeta$ and $Q$. 

Regarding the diluted correlation, corrective actions to adjust for it have been available since 1904 and continuously improved ever since [@spearman:1904:PMA; @frost:2000:C]. However, these methods are controversial as there is an overadjustment risk [@osborne:2002:ESD]. <!-- It could also create a p-hacking opportunity for researchers to deliberately deploy a lousy automated procedure and then use the adjustment to "concentrate" the found correlation. --> If these adjustment methods are to be used, they should be used exclusively for exploratory or sensitivity analyses [@hutcheon:2010:R]. In my opinion, the deployment of these adjustment methods should also be preregistered.

Instead of relying on these corrective actions, I advocate maintaining proper conduct. In an automated content analysis with proper conduct, the measurement errors are minimized and clearly documented. There are some excellent papers on how should (automated) content analyses be planned [@geiss2021statistical], executed [@atteveldt:2021:VSA;@barberá:2020:atc], and validated [@song:2020:ivw]. In light of the current finding, I would like to reinforce certain points from these papers by suggesting a few best practices.

## 1. Design automated content analysis carefully

Any deductive study needs a careful design, (automated) content analysis is no exception. @krippendorff2018content (specifically, Chapter 14), @lacy:2015:IBP, and @geiss2021statistical introduce how content analyses should be designed. I would like to add just two points. First, automated content analysis is about making valid inferences. Selecting a valid mode of statistical inference should also be a part of the research design. Population studies, which involve the entire population of text, do not allow frequentist inference. Alternative paradigms are needed: descriptive [@gerring:2012:MD], Bayesian[@western:1994:BIC], or graphical [@wickham:2010:G]. 

Second, @geiss2021statistical suggests considering $Q$, $n$, and $\rho$ in conjunction. In the realm of automated content analysis, there comes also $\zeta$. @geiss2021statistical suggest forming expectations regarding $\rho$ and $Q$ in design. Similarly, one should also form expectation regarding $\zeta$. There is a great uncertainty in estimating $\zeta$ before the study because it depends on many factors: availability of training and validating data, the technology used to create the predictive models, the availability of computer resources etc. The website paperswithcode.com lists the state-of-the-art $\zeta$ of machine learning models. But in practice, we rarely can attain those levels [see @atteveldt:2021:VSA for examples].

Third, @geiss2021statistical suggest preregistering the design together with these expectations. I concur.

## 2. Report cross tabulations, not just metrics

Metrics such as F1, CCR (for $\zeta$) and Krippendorff's $\alpha$ (for Q) are useful for guiding one's methodological decisions or for comparing methods. For $\zeta$, metrics such as F1 make assumptions about how $\zeta_{+}$ and $\zeta_{+}$ are averaged. F1, for example, has been criticized for generating misleading conclusions when the data are imbalanced [@chicco:2020:MMF]. In the situation of $P(X=1)$ being too high, a model with no talent in classifying true negative cases can still have a high F1, according to the equation \@ref(eq:ppv).

Figure \@ref(fig:simfig) displays the relationship between $\rho$ and the observed $P(X=1)$. In general, $P(X=1)$ increases with $\rho$. It is because $P(X=1|Y=1)$ needs to be larger than $P(X=1|Y=0)$ in order to obtain a large $\rho$. Therefore, when $\rho$ gets larger, the bias of F1 against true negative cases also gets larger. In those cases, the model does not need to know how to know how to classify true negative cases in order to get a high F1. Unlike F1, the amount of random measurement errors generated by a model is measured by both the misclassifications of true negative cases and true positive cases. Figure \@ref(fig:fixfig) displays the simulation results of fixed $\rho$ and $n$ but with varying levels of $\zeta_{+}$ and $\zeta_{-}$. A model needs to have both high $\zeta_{+}$ and $\zeta_{-}$ to obtain a relatively unbiased estimation of effect size. In medical research, it is common to report both $\zeta_{+}$ (true positive rate, also known as sensitivity or recall) and $\zeta_{-}$ (true negative rate, also known as specificity). Reporting of prevalence-neutral measurements such as positive and negative likelihood ratios is also common [@hayden:1999:LR].

```{r simfig, fig.cap = "The relationship between true effect size and observed prevalence of X. The line represents the local regression line", fig.height = 3}
knitr::include_graphics(here::here("analysis/sim_fig.pdf"))
```

```{r fixfig, fig.cap = "The relationship between true negative ratio (X-axis) and observed effect size (Y-axis) by true positive rate (column). The line represents the median of all simulated observed effect sizes and the ribbon represents its 95% interval estimation.", fig.height = 3}
scenarios <- readRDS(here::here("analysis/fixed_scenarios.RDS"))
scenarios %>% mutate(mdn = purrr::map_dbl(res, ~quantile(.$d2_phi, probs = 0.5, na.rm = TRUE)), pct25 = purrr::map_dbl(res, ~quantile(.$d2_phi, probs = 0.025, na.rm = TRUE)), pct975 = purrr::map_dbl(res, ~quantile(.$d2_phi, probs = 0.975, na.rm = TRUE))) %>% ggplot(aes(y = mdn, ymin = pct25, ymax = pct975, x = spec)) + geom_line() + geom_ribbon(alpha = 0.5) + facet_grid(.~sen) + theme_minimal() + xlab("Specificity") + ylab("Observed effect size (R)") + theme(axis.text.x = element_text(angle = 90)) + ylim(-.2, +.2) + labs(title = "True effect size = 0.2, n = 10 000")
```

Also drawing from medical research, the Standards for Reporting Diagnostic Accuracy (STARD) statement mandates the reporting of "Cross tabulation of the index test results (or their distribution) by the results of the reference standard" and "Estimates of diagnostic accuracy and their precision" [@bossuyt:2015:S]. Therefore, a medical diagnostic study needs to report accuracy metrics, their precision (e.g. 95\% Confidence Interval), and cross tabulations [See this paper for an example: @prince-guerra:2021:EAB]. This practice should be ported to communication research.

The cross tabulation, which is called Confusion Matrix in machine learning literature, complements metrics. It is also more useful than the so-called heatmap visualization of Confusion Matrix because cross tabulation provides raw numbers, not just color depths. The numbers are important because they provide clue on whether the predictive accuracy data are subjected to random sampling error. A measurement of $\zeta$ calculated with only a few cases is less accurate than the ones with a lot of cases. The provided R function `cal_bigphi` also supports these numbers (the parameters `tpn`, `tnn`, `Qn`, the denominators for calculating $\zeta_{+}$, $\zeta_{-}$, and $Q$ respectively) by assuming probabilities such as $\zeta_{+}$, $\zeta_{-}$, and $Q$ are drawn from a binomial distribution, $\mathcal{B}(n, p)$. When these probabilities are calculated with a larger sample size, there are more accurate and subjected to less random sampling error.

```r
cal_bigphi(rho = 0.0852, n = 2019,
           tpr = 0.831, tnr = 0.721,
           Q = 0.8964, tpn = 1000,
           tnn = 300, Qn = 400)
```

Regarding the number of cases for study $Q$ and $\zeta$, @song:2020:ivw advise researchers to "strive to increase the sizes of manually coded validation dataset as large as possible, preferably to more than N = 1,300 ..., assuming acceptable reliability (equal to or higher than .7)". I agree with this advice in principle but it is also important to get enough positive and negative cases so that both $\zeta_{+}$ and $\zeta_{-}$ are accurate. However, this cannot be achieved by deliberately oversampling positive or negative cases, because of the problem below.

## 3. Rule out systematic measurement error

It is also important to note that the subject matter in this paper is *random* measurement errors. And all the simulation conducted in this paper assumes that *systematic* measurement error is negligible. As stated in the Introduction section, systematic measurement error could influence both the variance and the average level of a variable, but the direction of influence depends on the composition of data.

Unlike the Monte Carlo simulation, systematic measurement error exists in real life and it's not common for (communication) researchers to report it. In a methodological research, @song:2020:ivw study one source of systematic measurement error: the non-random selection of cases for validating prediction models and such practice will generate bias in the estimation of $\zeta$. Therefore, it is inappropriate to oversample positive or negative cases to achieve higher $n$ in the validation procedure. $P(X)$ should be kept like the natural distribution in the entire dataset. Therefore, the valid way to increase the number of positive and negative cases is to increase the number of random samples, not deliberately select more positive or negative cases.

The composition of the cases for conducting the validation procedure is not the only source of systematic measurement error. Sometimes the error is not straightforward to detect and needs to be detected qualitatively. I recommend the error analysis process as in @atteveldt:2021:VSA to qualitatively document what kind of cases got misclassified.

# Coda

The entry point for automated content analysis is to scale up content analysis for the ever-increasing size of datasets [@trilling:2018:SCA;@lewis:2013:CAE]. In other words, automated content analysis allows researchers to *ob*tain more statistical power through increasing *n* and to study some extremely small effect sizes without the risk of Type II error. The findings from this study, however, underscores the problem in *re*tain those obtained statistical power due to the automation-induced measurement errors. When we do not look at the problem in terms of statistical power, which is only relevant to the frequentist inference, the ACIDIC problem underestimates the estimated effect sizes.

Obtaining more statistical power and obtaining unbiased effect sizes are like weddings. One cannot dance at two weddings at the same time. 

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
